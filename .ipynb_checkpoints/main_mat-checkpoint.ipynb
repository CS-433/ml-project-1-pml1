{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from training_procedure import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceFirstFeature(tx):\n",
    "    correct = tx[tx[:,0]!=-999][:,1:]\n",
    "    ncorrect = tx[tx[:,0]==-999][:, 1:]\n",
    "    xxN = build_poly(ncorrect, 2)\n",
    "    \n",
    "    xx = build_poly(correct, 2)\n",
    "    \n",
    "    xx_mean = np.mean(xx, axis=0)\n",
    "    xx_std = np.std(xx, axis=0)\n",
    "    xx = (xx - xx_mean) / xx_std\n",
    "    \n",
    "    w_init = np.zeros(xx.shape[1])\n",
    "    w, bestw, losses = train_first_feature(xx[:, 0], xx, w_init, 0.0001, 20, 0)\n",
    "    newFirst = xxN.dot(bestw)\n",
    "    print(tx.shape)\n",
    "    print(newFirst.shape)\n",
    "    print(ncorrect.shape)\n",
    "    print(np.insert(ncorrect, 0, newFirst, axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)\n",
    "\n",
    "# change outlier values to the mean without them\n",
    "mean = []\n",
    "for i in range(tx.shape[1]):\n",
    "    tx[:, i][tx[:, i] == -999] = np.mean(tx[:, i][tx[:, i] != -999])\n",
    "    mean.append(np.mean(tx[:, i][tx[:, i] != -999]))\n",
    "# transform the categorical features into one-hot features\n",
    "#tx_ = np.delete(tx, 22, axis = 1)\n",
    "#tx = np.hstack((tx,tx_**2,tx_**3,np.sin(tx_),np.cos(tx_),np.sin(2*tx_),np.cos(2*tx_),np.sin(tx_)**2,np.cos(tx_)**2,np.sin(0.5*tx_),np.cos(0.5*tx_),\n",
    "#    (tx[:, 22] == 0).astype(np.float64)[:, np.newaxis],\n",
    "#    (tx[:, 22] == 1).astype(np.float64)[:, np.newaxis],\n",
    "#    (tx[:, 22] == 2).astype(np.float64)[:, np.newaxis],\n",
    "#    (tx[:, 22] == 3).astype(np.float64)[:, np.newaxis]))\n",
    "ind = np.where(tx[:, 22] > 1)\n",
    "tx_ = np.delete(tx[tx[:, 22] > 1], 22, axis = 1)\n",
    "tx__ = np.hstack([tx[tx[:, 22] > 1],tx_[:,0].reshape((tx_.shape[0],1))*tx_,tx_[:,1].reshape((tx_.shape[0],1))*tx_,tx_[:,2].reshape((tx_.shape[0],1))*tx_,tx_[:,3].reshape((tx_.shape[0],1))*tx_,tx_[:,4].reshape((tx_.shape[0],1))*tx_,tx_[:,5].reshape((tx_.shape[0],1))*tx_,tx_[:,6].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,7].reshape((tx_.shape[0],1))*tx_,np.sin(tx_),tx_[:,8].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,9].reshape((tx_.shape[0],1))*tx_,tx_[:,10].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,11].reshape((tx_.shape[0],1))*tx_,tx_[:,12].reshape((tx_.shape[0],1))*tx_,tx_[:,13].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,14].reshape((tx_.shape[0],1))*tx_,tx_[:,15].reshape((tx_.shape[0],1))*tx_,tx_[:,16].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,17].reshape((tx_.shape[0],1))*tx_,tx_[:,18].reshape((tx_.shape[0],1))*tx_,tx_[:,19].reshape((tx_.shape[0],1))*tx_,\n",
    "                np.cos(tx_),np.sin(2*tx_),np.cos(2*tx_)])\n",
    "tx = np.hstack([tx_])\n",
    "tx = np.delete(tx, 22, axis = 1)\n",
    "\n",
    "# standardization\n",
    "tx_mean = np.mean(tx[:, 1:], axis=0)\n",
    "tx_std = np.std(tx[:, 1:], axis=0)\n",
    "tx[:, 1:] = (tx[:, 1:] - tx_mean) / tx_std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctOutliers(xi, lower, upper, newVals):\n",
    "    a = xi > upper\n",
    "    b = xi < lower\n",
    "    aorb = np.logical_or(a,b)\n",
    "    xxi = np.where(aorb, newVals, xi)\n",
    "    return xxi\n",
    "\n",
    "def handleOutliers(tx):\n",
    "    q3, q2, q1 = np.quantile(tx, [0.75, 0.5, 0.25], axis=0)\n",
    "    iqr = q3 - q1\n",
    "    upper = q3 + 1.5*iqr\n",
    "    lower = q1 - 1.5*iqr\n",
    "    print(np.argwhere(tx > upper).shape)\n",
    "    print(np.argwhere(tx < lower).shape)\n",
    "    #t2 = tx > upper\n",
    "    #tx_outliers = np.logical_or(tx > upper, tx < lower)\n",
    "    tx2 = np.apply_along_axis(correctOutliers, axis=1, arr=tx, lower=lower, upper=upper, newVals=q2)\n",
    "    return tx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52435, 2)\n",
      "(7378, 2)\n"
     ]
    }
   ],
   "source": [
    "newTx = handleOutliers(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaceFirstFeature(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "tx_mean = np.mean(tx[:, 1:], axis=0)\n",
    "tx_std = np.std(tx[:, 1:], axis=0)\n",
    "tx[:, 1:] = (tx[:, 1:] - tx_mean) / tx_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.shape, tx.shape\n",
    "np.mean(tx, axis = 0).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= least_squares_GD(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tx @ gd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Gradient descent accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gamma = 0.0005\n",
    "sgd_ws = np.zeros(tx.shape[1])\n",
    "\n",
    "# Initialization\n",
    "initial_w = sgd_ws\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_ws, sgd_losses= least_squares_SGD(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Stochastic Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Stochastic Gradient Descent: loss = {t}\".format(t=sgd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tx @ sgd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Stochastic gradient descent accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start least squares.\n",
    "start_time = datetime.datetime.now()\n",
    "ls_ws, ls_losses= least_squares(y[ind], tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Least Squares: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Least Squares: loss = {t}\".format(t=ls_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tx @ ls_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Least squares accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "degree = 1\n",
    "ratio = 0.8\n",
    "seed = None\n",
    "lambdas = np.logspace(-20, -15, 150)\n",
    "\n",
    "# select lambda\n",
    "lambda_ = hyperparameter_tuning_ridge_regression(tx, y[ind], degree, ratio, seed, lambdas)\n",
    "\n",
    "# Start ridge regression\n",
    "start_time = datetime.datetime.now()\n",
    "r_ws, r_losses = ridge_regression(y[ind], tx, lambda_)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge Regression: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Hyperparameter Selection: Lambda = {t}\".format(t=lambda_))\n",
    "print(\"Ridge Regression: loss = {t}\".format(t=r_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    return dimension: k-fold * interval\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)] \n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "\n",
    "# Initialization\n",
    "seed = None\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y[ind], k_fold, seed)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for k in range(k_fold):\n",
    "    val_y = y[k_indices[k, :]]\n",
    "    val_x = tx[k_indices[k, :]]\n",
    "    train_idx = np.vstack([k_indices[:k, :], k_indices[k+1:, :]]).flatten()\n",
    "    train_y = y[train_idx]\n",
    "    train_x = tx[train_idx]\n",
    "\n",
    "    print('------', k, 'fold ------')\n",
    "    w, loss = ridge_regression(train_y, train_x, lambda_)\n",
    "    pred = train_x @ w\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred < 0] = -1\n",
    "    train_acc.append(np.mean(pred == train_y))\n",
    "    print('Train acc:', np.mean(pred == train_y))\n",
    "    pred = val_x @ w\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred < 0] = -1\n",
    "    val_acc.append(np.mean(pred == val_y))\n",
    "    print('Validation acc:', np.mean(pred == val_y))\n",
    "\n",
    "print('------ summary ------')\n",
    "print('Average Train acc:', sum(train_acc)/len(train_acc))\n",
    "print('Average Validation acc:', sum(val_acc)/len(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "gamma = 0.0005\n",
    "\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= logistic_regression(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_logit(w, tx):\n",
    "    y_pred = sigmoid(np.dot(tx, w))\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    return y_pred\n",
    "# predictions\n",
    "y_pred = predict_labels_logit(gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
    "(y_pred == y[ind]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 3\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.00001\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= reg_logistic_regression(y[ind], tx,lambda_, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred = predict_labels_logit(gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
    "(y_pred == y[ind]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_logit_loss(y,tx,lambda_,gd_ws)\n",
    "#(sigmoid(tx@gd_ws)<0).sum()\n",
    "#tx@gd_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68114, 27)\n",
      "(68114,)\n",
      "(72543,)\n",
      "(72543, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:16<00:13,  1.54s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "#xx = build_poly(tx[:, 1:], 2)[:, 1:]\n",
    "xx = tx[tx[:,0]!=-999][:,1:]\n",
    "yy = tx[tx[:,0]!=-999][:,0]\n",
    "print(xx.shape)\n",
    "print(yy.shape)\n",
    "print(tx[:, 0].shape)\n",
    "print(tx[:, 1:].shape)\n",
    "w_init = np.zeros(xx.shape[1])\n",
    "train_first_feature(yy, xx, w_init, 0.0001, 20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8877ca64660a6d07633c323136e70418fcd1020e7ad0cd0d4c67d6a541235ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
