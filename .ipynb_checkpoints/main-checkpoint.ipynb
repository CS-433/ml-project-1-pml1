{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 4,
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import *\n",
    "from implementations import *\n",
<<<<<<< HEAD
    "from Data_Cleaning import *\n",
=======
    "from training_procedure import *\n",
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)\n",
    "#data_model = split_according_num_split_jet(y,tx,ids)"
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)\n"
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_34718/3361393657.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tx = (tx - tx_mean) / tx_std\n"
     ]
    }
   ],
   "source": [
    "tx = data_model[\"0\"][\"data\"]\n",
    "ind = data_model[\"0\"][\"ind\"]\n",
    "\n",
    "tx_mean = np.mean(tx, axis=0)\n",
    "tx_std = np.std(tx, axis=0)\n",
    "tx = (tx - tx_mean) / tx_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tx[tx[:,22]>1][:,27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
   "execution_count": 6,
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)\n",
    "\n",
    "# change outlier values to the mean without them\n",
    "mean = []\n",
    "for i in range(tx.shape[1]):\n",
    "    tx[:, i][tx[:, i] == -999] = np.mean(tx[:, i][tx[:, i] != -999])\n",
    "    mean.append(np.mean(tx[:, i][tx[:, i] != -999]))\n",
<<<<<<< HEAD
    "#print(tx[:,0].mean(),min(tx[:,0]))\n",
=======
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "# transform the categorical features into one-hot features\n",
    "#tx_ = np.delete(tx, 22, axis = 1)\n",
    "#tx = np.hstack((tx,tx_**2,tx_**3,np.sin(tx_),np.cos(tx_),np.sin(2*tx_),np.cos(2*tx_),np.sin(tx_)**2,np.cos(tx_)**2,np.sin(0.5*tx_),np.cos(0.5*tx_),\n",
    "#    (tx[:, 22] == 0).astype(np.float64)[:, np.newaxis],\n",
    "#    (tx[:, 22] == 1).astype(np.float64)[:, np.newaxis],\n",
    "#    (tx[:, 22] == 2).astype(np.float64)[:, np.newaxis],\n",
    "#    (tx[:, 22] == 3).astype(np.float64)[:, np.newaxis]))\n",
<<<<<<< HEAD
    "i = -1\n",
    "ind = np.where(tx[:, 22] > i)\n",
    "tx_ = np.delete(tx[tx[:, 22] > i], 22, axis = 1)\n",
    "#tx = np.hstack([tx[tx[:, 22] > 1],np.sin(tx_),np.cos(tx_)])#,np.sin(2*tx_),np.cos(2*tx_)])\n",
    "tx = np.delete(tx[tx[:, 22] > i], 22, axis = 1)\n",
    "#tx = build_derived_quantities(tx)\n",
    "tx = build_poly_cross_terms(tx, 3)\n",
=======
    "ind = np.where(tx[:, 22] > 1)\n",
    "tx_ = np.delete(tx[tx[:, 22] > 1], 22, axis = 1)\n",
    "tx__ = np.hstack([tx[tx[:, 22] > 1],tx_[:,0].reshape((tx_.shape[0],1))*tx_,tx_[:,1].reshape((tx_.shape[0],1))*tx_,tx_[:,2].reshape((tx_.shape[0],1))*tx_,tx_[:,3].reshape((tx_.shape[0],1))*tx_,tx_[:,4].reshape((tx_.shape[0],1))*tx_,tx_[:,5].reshape((tx_.shape[0],1))*tx_,tx_[:,6].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,7].reshape((tx_.shape[0],1))*tx_,np.sin(tx_),tx_[:,8].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,9].reshape((tx_.shape[0],1))*tx_,tx_[:,10].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,11].reshape((tx_.shape[0],1))*tx_,tx_[:,12].reshape((tx_.shape[0],1))*tx_,tx_[:,13].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,14].reshape((tx_.shape[0],1))*tx_,tx_[:,15].reshape((tx_.shape[0],1))*tx_,tx_[:,16].reshape((tx_.shape[0],1))*tx_,\n",
    "                tx_[:,17].reshape((tx_.shape[0],1))*tx_,tx_[:,18].reshape((tx_.shape[0],1))*tx_,tx_[:,19].reshape((tx_.shape[0],1))*tx_,\n",
    "                np.cos(tx_),np.sin(2*tx_),np.cos(2*tx_)])\n",
    "tx = np.hstack([tx_])\n",
    "tx = np.delete(tx, 22, axis = 1)\n",
    "\n",
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "# standardization\n",
    "tx_mean = np.mean(tx, axis=0)\n",
    "tx_std = np.std(tx, axis=0)\n",
    "tx = (tx - tx_mean) / tx_std\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 7,
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "((250000,), (2,))"
      ]
     },
     "execution_count": 28,
=======
       "(28,)"
      ]
     },
     "execution_count": 7,
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "y.shape, tx.shape\n",
    "#np.mean(tx, axis = 0).shape\n"
=======
    "#y.shape, tx.shape\n",
    "np.mean(tx, axis = 0).shape\n"
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time = 2.890 seconds\n",
      "Gradient Descent: loss = 0.2546915291394869\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gamma = 0.0003\n",
    "\n",
    "# Initialization\n",
    "initial_w = ls_ws#np.zeros(tx.shape[1])\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx.shape[1])\n",
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= least_squares_GD(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent accuracy: 0.8358077278303903\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "y_pred = tx @ gd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Gradient descent accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent: execution time = 73.331 seconds\n",
      "Stochastic Gradient Descent: loss = 0.26469060718689935\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "initial_w = ls_ws#sgd_ws#np.zeros(tx.shape[1])\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gamma = 0.0005\n",
    "sgd_ws = np.zeros(tx.shape[1])\n",
    "\n",
    "# Initialization\n",
    "initial_w = sgd_ws\n",
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_ws, sgd_losses= least_squares_SGD(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Stochastic Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Stochastic Gradient Descent: loss = {t}\".format(t=sgd_losses))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic gradient descent accuracy: 0.8331885915939512\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "y_pred = tx @ sgd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Stochastic gradient descent accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares: execution time = 185.925 seconds\n",
      "Least Squares: loss = 1.4513104762904974\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "# Start least squares.\n",
    "start_time = datetime.datetime.now()\n",
    "ls_ws, ls_losses= least_squares(y[ind], tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Least Squares: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Least Squares: loss = {t}\".format(t=ls_losses))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares accuracy: 0.708364\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "y_pred = tx @ ls_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Least squares accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression: execution time = 0.006 seconds\n",
      "Hyperparameter Selection: Lambda = 1e-15\n",
      "Ridge Regression: loss = 0.3655438801671611\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "# initialization\n",
    "degree = 1\n",
    "ratio = 0.8\n",
    "seed = None\n",
    "lambdas = np.logspace(-20, -15, 150)\n",
    "\n",
    "# select lambda\n",
    "lambda_ = hyperparameter_tuning_ridge_regression(tx, y[ind], degree, ratio, seed, lambdas)\n",
    "\n",
    "# Start ridge regression\n",
    "start_time = datetime.datetime.now()\n",
    "r_ws, r_losses = ridge_regression(y[ind], tx, lambda_)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge Regression: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Hyperparameter Selection: Lambda = {t}\".format(t=lambda_))\n",
    "print(\"Ridge Regression: loss = {t}\".format(t=r_losses))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": null,
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    return dimension: k-fold * interval\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)] \n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ 0 fold ------\n",
      "Train acc: 0.5171284808381582\n",
      "Validation acc: 0.4951750758202371\n",
      "------ 1 fold ------\n",
      "Train acc: 0.5140267438654536\n",
      "Validation acc: 0.4986903777226358\n",
      "------ 2 fold ------\n",
      "Train acc: 0.519696029776675\n",
      "Validation acc: 0.4895230217810863\n",
      "------ 3 fold ------\n",
      "Train acc: 0.5168010752688172\n",
      "Validation acc: 0.478494623655914\n",
      "------ 4 fold ------\n",
      "Train acc: 0.5183691756272402\n",
      "Validation acc: 0.47435897435897434\n",
      "------ summary ------\n",
      "Average Train acc: 0.5172043010752689\n",
      "Average Validation acc: 0.48724841466776947\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "# Cross Validation\n",
    "\n",
    "# Initialization\n",
    "seed = None\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y[ind], k_fold, seed)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for k in range(k_fold):\n",
<<<<<<< HEAD
    "    val_y = y[ind][k_indices[k, :]]\n",
=======
    "    val_y = y[k_indices[k, :]]\n",
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "    val_x = tx[k_indices[k, :]]\n",
    "    train_idx = np.vstack([k_indices[:k, :], k_indices[k+1:, :]]).flatten()\n",
    "    train_y = y[train_idx]\n",
    "    train_x = tx[train_idx]\n",
    "\n",
    "    print('------', k, 'fold ------')\n",
    "    w, loss = ridge_regression(train_y, train_x, lambda_)\n",
    "    pred = train_x @ w\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred < 0] = -1\n",
    "    train_acc.append(np.mean(pred == train_y))\n",
    "    print('Train acc:', np.mean(pred == train_y))\n",
    "    pred = val_x @ w\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred < 0] = -1\n",
    "    val_acc.append(np.mean(pred == val_y))\n",
    "    print('Validation acc:', np.mean(pred == val_y))\n",
    "\n",
    "print('------ summary ------')\n",
    "print('Average Train acc:', sum(train_acc)/len(train_acc))\n",
    "print('Average Validation acc:', sum(val_acc)/len(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time = 11.558 seconds\n",
      "Gradient Descent: loss = inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranvaudaux/Desktop/Machine-Learning-Project/implementations.py:150: RuntimeWarning: divide by zero encountered in log\n",
      "  X_tx = tx@w\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "gamma = 0.0005\n",
    "\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
<<<<<<< HEAD
    "gd_ws, gd_losses= logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
=======
    "gd_ws, gd_losses= logistic_regression(y[ind], tx, initial_w, max_iters, gamma)\n",
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_17851/2333611001.py:10: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  (y_pred == y).mean()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predict_labels_logit(gd_ws, tx)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# accuracy\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'mean'"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "def predict_labels_logit(w, tx):\n",
    "    y_pred = sigmoid(np.dot(tx, w))\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    return y_pred\n",
    "# predictions\n",
    "y_pred = predict_labels_logit(gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
<<<<<<< HEAD
    "(y_pred == y).mean()"
=======
    "(y_pred == y[ind]).mean()"
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
<<<<<<< HEAD
    "max_iters = 4\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.000001\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.random.normal(scale = 1/np.sqrt(tx.shape[0]), size=(tx.shape[1]))\n",
=======
    "max_iters = 3\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.00001\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= reg_logistic_regression(y[ind], tx,lambda_, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7225094082130599"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "source": [
    "# predictions\n",
    "y_pred = predict_labels_logit(gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
    "(y_pred == y[ind]).mean()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 193,
=======
   "execution_count": null,
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_logit_loss(y,tx,lambda_,gd_ws)\n",
    "#(sigmoid(tx@gd_ws)<0).sum()\n",
    "#tx@gd_ws"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_89830/608895844.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ids = x[:, 0].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "x = np.genfromtxt(\"test.csv\", delimiter=\",\", skip_header=1)\n",
    "ids = x[:, 0].astype(np.int)\n",
    "txx = x[:, 2:]\n",
    "    \n",
    "for i in range(txx.shape[1]):\n",
    "    txx[:, i][txx[:, i] == -999] = mean[i]\n",
    "# transform the categorical features into one-hot features\n",
    "txx_= np.delete(txx, 22, axis = 1)\n",
    "txx = np.hstack((txx,txx_**2,np.sin(txx_),np.cos(txx_),np.sin(2*txx_),np.cos(2*txx_),\n",
    "        (txx[:, 22] == 0).astype(np.float64)[:, np.newaxis],\n",
    "        (txx[:, 22] == 1).astype(np.float64)[:, np.newaxis],\n",
    "        (txx[:, 22] == 2).astype(np.float64)[:, np.newaxis],\n",
    "        (txx[:, 22] == 3).astype(np.float64)[:, np.newaxis]))\n",
    "txx = np.delete(txx, 22, axis = 1)\n",
    "\n",
    "    # standardization\n",
    "#txx_mean = np.mean(txx, axis=0)\n",
    "#txx_std = np.std(txx, axis=0)\n",
    "txx[:, :-4] = (txx[:, :-4] - tx_mean[:-4]) / tx_std[:-4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels_logit(gd_ws, txx).astype(int)\n",
    "y_pred[y_pred==0]=-1\n",
    "print(np.hstack((ids.reshape(len(ids),1),y_pred.reshape(len(ids),1))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "d = pd.DataFrame(np.hstack((ids.reshape(len(ids),1),y_pred.reshape(len(ids),1))), columns = [\"Id\",\"Prediction\"])\n",
    "d.to_csv(\"submission_test_1.csv\",index=False)"
=======
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.35898036e-01  4.36714239e-01 -1.08552886e+00 ...  2.33913276e-01\n",
      "  -2.52274089e+00 -2.61630915e-01]\n",
      " [-7.16897628e-01 -4.66483417e-01  8.34344563e-02 ...  1.56417161e-03\n",
      "   5.00320342e+00  1.17633427e-03]\n",
      " [-2.93845346e-01  6.69180568e-01 -5.16860831e-02 ...  3.47536529e-04\n",
      "  -3.52583892e+00 -1.14750561e-05]\n",
      " ...\n",
      " [-3.90801233e-02  1.56653721e+01 -4.25443729e-01 ...  1.74362128e-02\n",
      "   1.21396582e-02  7.72370005e-03]\n",
      " [-9.83018290e-01 -2.88263123e-01 -4.32164038e-01 ...  2.44690557e-02\n",
      "  -1.80788329e+00  3.86592888e+01]\n",
      " [ 2.11596036e-01 -3.80665862e-01  2.37009387e-01 ... -1.17233499e-05\n",
      "  -2.44032259e-01 -4.31196488e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.98131962e-08,  8.20139676e-08,  1.45063893e-09, -2.64758043e-09,\n",
       "        -2.09601422e-09,  1.78308093e-09,  5.00082827e-08,  1.63811912e-09,\n",
       "         8.33562928e-09,  6.92866962e-09, -1.49513018e-09, -3.26661317e-09,\n",
       "         1.69870155e-08, -4.30538319e-11, -3.17261426e-10,  2.38228851e-08,\n",
       "         3.77566182e-10, -7.72439474e-11,  7.53528593e-09, -7.71210140e-11,\n",
       "         5.74399385e-09,  6.42783109e-10,  2.28646085e-10, -1.56208271e-09,\n",
       "         4.84589468e-10, -1.88206218e-10, -1.05055349e-09,  4.97905603e-08,\n",
       "         3.29207138e-07,  4.14832706e-10,  1.39961133e-10, -8.57044836e-10,\n",
       "         2.41958143e-10,  5.60166771e-08,  1.08409318e-09,  5.07616692e-09,\n",
       "         3.25189042e-08,  7.68689372e-09,  8.63277035e-10,  4.27946547e-08,\n",
       "         1.02944612e-08, -2.32240962e-11,  5.82563894e-08,  7.81402740e-09,\n",
       "         1.85218453e-10,  1.05844029e-08, -1.40076436e-11,  2.95528610e-09,\n",
       "        -7.18993068e-10,  9.20052283e-11, -6.16375961e-09, -1.63449129e-09,\n",
       "        -5.73826086e-10, -2.45860173e-09,  2.67100382e-07,  2.50594761e-06,\n",
       "         1.56681102e-08, -3.35192643e-09, -5.35262007e-09,  3.14794173e-09,\n",
       "         2.36459534e-07,  3.56226835e-10,  2.88677305e-08,  1.68808009e-07,\n",
       "        -1.28281684e-08, -4.32149478e-09,  2.78052356e-07, -5.09665891e-10,\n",
       "         4.57407462e-11,  3.24824280e-07,  2.26430584e-09, -5.35054095e-10,\n",
       "         7.30809855e-08,  2.19183939e-10,  1.51710914e-08,  1.53545771e-09,\n",
       "        -3.93527252e-10, -4.96578496e-08,  1.15683211e-09, -1.68658168e-10,\n",
       "        -1.57077350e-08]),\n",
       " array([ 1.98131962e-08,  8.20139676e-08,  1.45063893e-09, -2.64758043e-09,\n",
       "        -2.09601422e-09,  1.78308093e-09,  5.00082827e-08,  1.63811912e-09,\n",
       "         8.33562928e-09,  6.92866962e-09, -1.49513018e-09, -3.26661317e-09,\n",
       "         1.69870155e-08, -4.30538319e-11, -3.17261426e-10,  2.38228851e-08,\n",
       "         3.77566182e-10, -7.72439474e-11,  7.53528593e-09, -7.71210140e-11,\n",
       "         5.74399385e-09,  6.42783109e-10,  2.28646085e-10, -1.56208271e-09,\n",
       "         4.84589468e-10, -1.88206218e-10, -1.05055349e-09,  4.97905603e-08,\n",
       "         3.29207138e-07,  4.14832706e-10,  1.39961133e-10, -8.57044836e-10,\n",
       "         2.41958143e-10,  5.60166771e-08,  1.08409318e-09,  5.07616692e-09,\n",
       "         3.25189042e-08,  7.68689372e-09,  8.63277035e-10,  4.27946547e-08,\n",
       "         1.02944612e-08, -2.32240962e-11,  5.82563894e-08,  7.81402740e-09,\n",
       "         1.85218453e-10,  1.05844029e-08, -1.40076436e-11,  2.95528610e-09,\n",
       "        -7.18993068e-10,  9.20052283e-11, -6.16375961e-09, -1.63449129e-09,\n",
       "        -5.73826086e-10, -2.45860173e-09,  2.67100382e-07,  2.50594761e-06,\n",
       "         1.56681102e-08, -3.35192643e-09, -5.35262007e-09,  3.14794173e-09,\n",
       "         2.36459534e-07,  3.56226835e-10,  2.88677305e-08,  1.68808009e-07,\n",
       "        -1.28281684e-08, -4.32149478e-09,  2.78052356e-07, -5.09665891e-10,\n",
       "         4.57407462e-11,  3.24824280e-07,  2.26430584e-09, -5.35054095e-10,\n",
       "         7.30809855e-08,  2.19183939e-10,  1.51710914e-08,  1.53545771e-09,\n",
       "        -3.93527252e-10, -4.96578496e-08,  1.15683211e-09, -1.68658168e-10,\n",
       "        -1.57077350e-08]),\n",
       " [0.49997969720854635,\n",
       "  0.49997293467412995,\n",
       "  0.49996617466942966,\n",
       "  0.49995941719345965,\n",
       "  0.4999526622452344,\n",
       "  0.49994590982376935,\n",
       "  0.49993915992807947,\n",
       "  0.49993241255718085])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xx = build_poly(tx[:, 1:], 3)[:, 1:]\n",
    "print(xx)\n",
    "w_init = np.zeros(xx.shape[1])\n",
    "train_first_feature(tx[:, 0], xx, w_init, 0.0001, 10, 0)"
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "op_axes must be a tuple/list matching the number of ops",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcombinations_with_replacement(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mop_axes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ind)\n",
      "\u001b[0;31mValueError\u001b[0m: op_axes must be a tuple/list matching the number of ops"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for ind in itertools.combinations_with_replacement(np.nditer(tx,op_axes = 0),2):\n",
    "    print(ind)"
   ]
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.12"
=======
   "version": "3.9.7"
>>>>>>> b28f5b23757933c22dbdea7f54da4fb92f67231a
  },
  "vscode": {
   "interpreter": {
    "hash": "a8877ca64660a6d07633c323136e70418fcd1020e7ad0cd0d4c67d6a541235ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
