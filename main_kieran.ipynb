{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from Data_Cleaning import *\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('information_transformation_final.json', 'rb') as fp:\n",
    "        inf = pickle.load(fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'mean_log': array([ 4.63880110e+00,  3.85486354e+00,  4.21944559e+00,  2.15629857e+00,\n",
       "         -9.99000000e+02, -9.99000000e+02, -9.99000000e+02,  2.66496128e+00,\n",
       "          2.15629855e+00,  3.20836222e+00,  7.86852084e-01, -9.10076857e-01,\n",
       "         -9.99000000e+02,  2.26080610e+00, -2.48576361e-02, -1.56573619e-02,\n",
       "          2.50449099e+00, -5.23114410e-02,  4.23519862e-02,  3.29970635e+00,\n",
       "         -2.44434558e-02,  4.61018539e+00,  0.00000000e+00, -9.99000000e+02,\n",
       "         -9.99000000e+02, -9.99000000e+02, -9.99000000e+02, -9.99000000e+02,\n",
       "         -9.99000000e+02,  0.00000000e+00,  8.30512546e-01]),\n",
       "  'sigma_log': array([0.35516785, 0.84175586, 0.48050508, 1.10675864, 0.        ,\n",
       "         0.        , 0.        , 0.69329161, 1.10675867, 0.71398276,\n",
       "         0.24761112, 0.93670201, 0.        , 0.99749166, 1.23342423,\n",
       "         1.81734217, 0.90557477, 1.31084863, 1.81783473, 0.6504099 ,\n",
       "         1.81099743, 0.51532302, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.3751819 ]),\n",
       "  'mean': array([-1.07252724e-01, -4.48797964e-01,  6.67483929e+01, ...,\n",
       "          1.94032973e-03,  1.09931699e-03,  1.40869680e-03]),\n",
       "  'sigma': array([26.48617469, 26.2120496 , 49.72404155, ...,  0.45733053,\n",
       "          0.45842529,  0.46259764]),\n",
       "  'median': 111.452},\n",
       " '1': {'mean_log': array([ 4.62963721e+00,  3.45854272e+00,  4.23271232e+00,  4.00825011e+00,\n",
       "         -9.99000000e+02, -9.99000000e+02, -9.99000000e+02,  2.33968637e+00,\n",
       "          2.34514115e+00,  4.02687285e+00,  8.06122539e-01,  2.35611975e-01,\n",
       "         -9.99000000e+02,  2.48847106e+00, -1.73527288e-03, -7.31197772e-03,\n",
       "          2.64352649e+00,  6.68554627e-03,  4.48938280e-02,  3.49995902e+00,\n",
       "         -5.57938719e-03,  5.12107445e+00,  1.00000000e+00,  2.95447911e+00,\n",
       "         -8.00435882e-04, -1.50522284e-02, -9.99000000e+02, -9.99000000e+02,\n",
       "         -9.99000000e+02,  2.95447915e+00,  3.50278552e-01]),\n",
       "  'sigma_log': array([0.43485003, 1.03582434, 0.46400954, 0.64417214, 0.        ,\n",
       "         0.        , 0.        , 0.73762801, 1.11023158, 0.75467963,\n",
       "         0.315949  , 1.1063327 , 0.        , 1.04351985, 1.21818737,\n",
       "         1.81444814, 0.99980748, 1.26220634, 1.8154439 , 0.67949649,\n",
       "         1.81192526, 0.4273415 , 0.        , 1.1925549 , 1.82158106,\n",
       "         1.81751683, 0.        , 0.        , 0.        , 1.19255486,\n",
       "         0.47705711]),\n",
       "  'mean': array([-2.14041395e-02, -3.32537236e-01,  7.38309195e+01, ...,\n",
       "          8.13285428e-04, -1.65934157e-03, -1.00338698e-04]),\n",
       "  'sigma': array([31.06033858, 31.07277595, 57.46911588, ...,  0.46380155,\n",
       "          0.45575892,  0.42039367]),\n",
       "  'median': 112.4055},\n",
       " '2': {'mean_log': array([ 4.64675850e+00,  3.24749554e+00,  4.17668481e+00,  4.47160650e+00,\n",
       "          1.08471585e+00,  5.37708865e+00, -8.21688171e-01,  2.00683794e+00,\n",
       "          2.79060703e+00,  4.88386003e+00,  8.18092991e-01,  5.59420440e-01,\n",
       "          4.58289801e-01,  2.73807985e+00, -1.72450822e-03,  1.22142729e-03,\n",
       "          2.81600031e+00, -2.32548971e-03,  4.37392994e-02,  3.81015708e+00,\n",
       "          4.75675117e-03,  5.65139425e+00,  2.30552913e+00,  3.98993936e+00,\n",
       "         -5.91930303e-03, -9.55008753e-03,  2.83425109e+00, -1.18452642e-02,\n",
       "         -1.58228913e-03,  4.45097583e+00,  6.94470866e-01,  3.05529134e-01,\n",
       "          2.04719959e-01,  4.99951753e-01]),\n",
       "  'sigma_log': array([0.39253267, 1.07325222, 0.45467233, 0.74115602, 0.54430863,\n",
       "         1.03441954, 3.58433731, 0.78491446, 1.22068851, 0.73768669,\n",
       "         0.37180066, 0.96775279, 0.39867861, 1.09974678, 1.18221374,\n",
       "         1.81837922, 1.06953167, 1.20101053, 1.8161594 , 0.74815993,\n",
       "         1.81406907, 0.41398648, 0.46063118, 0.88881116, 1.74407185,\n",
       "         1.808942  , 1.09239586, 2.0317286 , 1.8169372 , 0.91900751,\n",
       "         0.46063118, 0.46063118, 0.40349684, 0.5       ]),\n",
       "  'mean': array([-1.04395103e-01, -1.16328178e-01,  8.29591474e+01, ...,\n",
       "         -4.84311636e-03,  8.34050854e-04, -1.12454738e-03]),\n",
       "  'sigma': array([38.26025051, 38.13524978, 69.19375522, ...,  0.67538275,\n",
       "          0.70740848,  0.70919284]),\n",
       "  'median': 113.23}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranvaudaux/Desktop/ml-project-1-pml1/Data_Cleaning.py:58: RuntimeWarning: invalid value encountered in true_divide\n",
      "  X_ =(X_ - m)/sigma\n"
     ]
    }
   ],
   "source": [
    "data_model, information_transformation_final = split_according_num_split_jet(y,tx,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez('information_transformation_final.npz',information_transformation_final)\n",
    "#data=np.load('temp.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72543, 3710)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = data_model[\"2\"][\"data\"]\n",
    "y_ = data_model[\"2\"][\"label\"]\n",
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 1/10000 [00:02<6:23:44,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 0, the loss is 1.317171 while the best loss is 1.317171.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.7199454116868615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                     | 501/10000 [02:28<56:09,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 500, the loss is 0.268012 while the best loss is 0.268012.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.827550556221827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                  | 1001/10000 [04:57<48:20,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 1000, the loss is 0.263288 while the best loss is 0.263288.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8299215637621824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 1501/10000 [07:23<44:33,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 1500, the loss is 0.261022 while the best loss is 0.261022.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8308589388362764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▌                              | 2001/10000 [09:49<42:49,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 2000, the loss is 0.259639 while the best loss is 0.259639.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8315068304316061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 2501/10000 [12:16<39:49,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 2500, the loss is 0.258687 while the best loss is 0.258687.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8320995823166949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▍                          | 3001/10000 [15:57<39:44,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 3000, the loss is 0.257983 while the best loss is 0.257983.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8325406999986215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████▎                        | 3501/10000 [18:31<41:02,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 3500, the loss is 0.257436 while the best loss is 0.257436.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8327888286947052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▏                      | 4001/10000 [21:12<35:01,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 4000, the loss is 0.256993 while the best loss is 0.256993.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.833174806666391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████                     | 4501/10000 [23:48<40:37,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 4500, the loss is 0.256625 while the best loss is 0.256625.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8331472368112706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████                   | 5001/10000 [26:29<29:49,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 5000, the loss is 0.256313 while the best loss is 0.256313.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8332713011593124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████▉                 | 5501/10000 [29:10<29:53,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 5500, the loss is 0.256042 while the best loss is 0.256042.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8335194298553961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████▊               | 6001/10000 [31:58<24:28,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 6000, the loss is 0.255804 while the best loss is 0.255804.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8338778379719615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████▋             | 6501/10000 [34:44<21:25,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 6500, the loss is 0.255592 while the best loss is 0.255592.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8338226982617206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████▌           | 7001/10000 [37:30<18:13,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 7000, the loss is 0.255401 while the best loss is 0.255401.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8339191927546421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████▌         | 7501/10000 [40:15<15:06,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 7500, the loss is 0.255228 while the best loss is 0.255228.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.834112181740485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████▍       | 8001/10000 [43:01<12:06,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 8000, the loss is 0.255070 while the best loss is 0.255070.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8343327405814482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████▎     | 8501/10000 [45:48<09:04,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 8500, the loss is 0.254924 while the best loss is 0.254924.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8344154501468095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████▏   | 9001/10000 [48:30<05:44,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 9000, the loss is 0.254790 while the best loss is 0.254790.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8344016652192493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████  | 9501/10000 [51:16<03:01,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "At iteration 9500, the loss is 0.254665 while the best loss is 0.254665.\n",
      "----------------------------------------------------\n",
      "Accuracy: 0.8343878802916891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [54:00<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training procedure: execution time = 3240.635 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "gamma = 0.05\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "best_w, w, losses = training_procedure(y_, X_, gamma, initial_w = \"Gaussian\", type_ = \"GD\",\n",
    "                                       num_iterations = epochs, increase_limit = 3, verbose = True,\n",
    "                                       lambda_ = None)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Training procedure: execution time = {t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('best_w_2_default_indicatrice.npy',best_w)\n",
    "np.save('losses_2_default_indicatrice.npy',losses)\n",
    "np.save('last_w_2_default_indicatrice.npy',w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.8346773637704534\n",
      "Last accuracy: 0.8346773637704534\n"
     ]
    }
   ],
   "source": [
    "y_pred = X_ @ best_w\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Best accuracy:', np.mean(y_pred == y_))\n",
    "\n",
    "y_pred = X_ @ w\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Last accuracy:', np.mean(y_pred == y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fig, axs = plt.subplots(tx.shape[1],1,figsize =(5,50))\n",
    "#fig.tight_layout(pad=2.0)\n",
    "\n",
    "#for i in range(tx.shape[1]):\n",
    "    #print(np.median(tx[:, i][tx[:, i] != -999]))\n",
    "    #w=2\n",
    "   # print(i,\":  \",min(tx[(tx[:, 22]==w), i][tx[(tx[:, 22]==w), i] != -999]),\"   \", max(tx[(tx[:, 22]==w), i][tx[tx[:, 22]==w, i] != -999]))\n",
    "    #tx[:, i][tx[:, i] == -999] = np.median(tx[:, i][tx[:, i] != -999])\n",
    "    #axs[i].boxplot(tx[(tx[:, 22]==w),i])\n",
    "    #axs[i].hist(tx[tx[:,i]!=-999,i],bins=100)\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3360"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.hist(np.log(tx[:,10]),bins=100)\n",
    "#np.median(tx[tx[:,11]!=-999,11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)\n",
    "#for i in range(tx.shape[1]):\n",
    " #   tx[:, i][tx[:, i] == -999] = np.median(tx[:, i][tx[:, i] != -999])\n",
    "data_model = split_according_num_split_jet(y,tx,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4606311783304026 0\n"
     ]
    }
   ],
   "source": [
    "tx = data_model[\"2\"][\"data\"]\n",
    "ind = data_model[\"2\"][\"ind\"]\n",
    "y = data_model[\"2\"][\"label\"]\n",
    "\n",
    "#tx_mean = np.mean(tx, axis=0)\n",
    "#tx_std = np.std(tx, axis=0)\n",
    "#tx = (tx - tx_mean) / tx_std\n",
    "#print(min(tx_std),(tx_std<1e-20).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72543, 657)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time = 0.315 seconds\n",
      "Gradient Descent: loss = 0.37866975281916476\n",
      "Gradient descent accuracy: 0.7083936423914092\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.005\n",
    "\n",
    "# Initialiation\n",
    "initial_w = gd_ws# np.zeros(tx.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= least_squares_GD(y, tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))\n",
    "\n",
    "y_pred = tx @ gd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Gradient descent accuracy:', np.mean(y_pred == y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent accuracy: 0.728773341586712\n"
     ]
    }
   ],
   "source": [
    "# cross validation for GD. Maybe No Need...\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "k_fold = 5\n",
    "seed = None\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "max_iters = 10 \n",
    "gammas = np.logspace(-4, -2, 20)\n",
    "\n",
    "# select gamma\n",
    "gamma_ = hyperparameter_tuning_GD(tx, y, k_fold, seed, initial_w, max_iters, gammas)\n",
    "\n",
    "# Start gd\n",
    "gd_ws, gd_losses = least_squares_GD(y, tx, initial_w, max_iters, gamma_)\n",
    "\n",
    "# Print result\n",
    "print(\"Hyperparameter Selection: Gamma = {t}\".format(t=gamma_))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))\n",
    "\n",
    "y_pred = tx @ gd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Gradient descent accuracy:', np.mean(y_pred == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent: execution time = 10.384 seconds\n",
      "Stochastic Gradient Descent: loss = nan\n",
      "Stochastic gradient descent accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "gamma = 0.000000015\n",
    "sgd_ws = np.zeros(tx.shape[1])\n",
    "\n",
    "# Initialization\n",
    "initial_w = sgd_ws\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_ws, sgd_losses= least_squares_SGD(y, tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Stochastic Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Stochastic Gradient Descent: loss = {t}\".format(t=sgd_losses))\n",
    "\n",
    "y_pred = tx @ sgd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Stochastic gradient descent accuracy:', np.mean(y_pred == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares: execution time = 0.304 seconds\n",
      "Least Squares: loss = 1.9140650918744522\n",
      "Least squares accuracy: 0.5550501082116813\n"
     ]
    }
   ],
   "source": [
    "# Start least squares.\n",
    "start_time = datetime.datetime.now()\n",
    "ls_ws, ls_losses= least_squares(y, tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Least Squares: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Least Squares: loss = {t}\".format(t=ls_losses))\n",
    "\n",
    "y_pred = tx @ ls_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Least squares accuracy:', np.mean(y_pred == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m lambdas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# select lambda\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m lambda_ \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_tuning_ridge_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Start ridge regression\u001b[39;00m\n\u001b[1;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m~/Desktop/ml-project-1-pml1/implementations.py:136\u001b[0m, in \u001b[0;36mhyperparameter_tuning_ridge_regression\u001b[0;34m(x, y, degree, ratio, seed, lambdas)\u001b[0m\n\u001b[1;32m    134\u001b[0m mse_te \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, lambda_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lambdas):\n\u001b[0;32m--> 136\u001b[0m     w, _ \u001b[38;5;241m=\u001b[39m \u001b[43mridge_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     mse_te\u001b[38;5;241m.\u001b[39mappend(compute_mse(y_te, tx_te, w))\n\u001b[1;32m    139\u001b[0m index_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(mse_te)\n",
      "File \u001b[0;32m~/Desktop/ml-project-1-pml1/implementations.py:84\u001b[0m, in \u001b[0;36mridge_regression\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m     82\u001b[0m a \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(tx) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m tx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m lambda_ \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(tx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     83\u001b[0m b \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(y)\n\u001b[0;32m---> 84\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m mse \u001b[38;5;241m=\u001b[39m compute_mse(y, tx, w)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w, mse\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/linalg/linalg.py:393\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    391\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    392\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 393\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/linalg/linalg.py:88\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "degree = 1\n",
    "ratio = 0.8\n",
    "seed = None\n",
    "lambdas = np.logspace(-20, -15, 150)\n",
    "\n",
    "# select lambda\n",
    "lambda_ = hyperparameter_tuning_ridge_regression(tx, y, degree, ratio, seed, lambdas)\n",
    "\n",
    "# Start ridge regression\n",
    "start_time = datetime.datetime.now()\n",
    "r_ws, r_losses = ridge_regression(y, tx, lambda_)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge Regression: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Ridge Regression: loss = {t}\".format(t=r_losses))\n",
    "\n",
    "y_pred = tx @ r_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Ridge Regression accuracy:', np.mean(y_pred == y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialization\n",
    "# degree = 1\n",
    "# ratio = 0.8\n",
    "# seed = None\n",
    "# lambdas = np.logspace(-20, -15, 150)\n",
    "\n",
    "# # select lambda\n",
    "# lambda_ = hyperparameter_tuning_ridge_regression(tx, y[ind], degree, ratio, seed, lambdas)\n",
    "\n",
    "# # Start ridge regression\n",
    "# start_time = datetime.datetime.now()\n",
    "# r_ws, r_losses = ridge_regression(y[ind], tx, lambda_)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "# # Print result\n",
    "# exection_time = (end_time - start_time).total_seconds()\n",
    "# print(\"Ridge Regression: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "# print(\"Hyperparameter Selection: Lambda = {t}\".format(t=lambda_))\n",
    "# print(\"Ridge Regression: loss = {t}\".format(t=r_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_k_indices(y, k_fold, seed):\n",
    "#     \"\"\"build k indices for k-fold.\n",
    "#     return dimension: k-fold * interval\n",
    "#     \"\"\"\n",
    "#     num_row = y.shape[0]\n",
    "#     interval = int(num_row / k_fold)\n",
    "#     np.random.seed(seed)\n",
    "#     indices = np.random.permutation(num_row)\n",
    "#     k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)] \n",
    "#     return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross Validation\n",
    "\n",
    "# Initialization\n",
    "seed = None\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y[ind], k_fold, seed)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for k in range(k_fold):\n",
    "    val_y = y[ind][k_indices[k, :]]\n",
    "    val_x = tx[k_indices[k, :]]\n",
    "    train_idx = np.vstack([k_indices[:k, :], k_indices[k+1:, :]]).flatten()\n",
    "    train_y = y[train_idx]\n",
    "    train_x = tx[train_idx]\n",
    "\n",
    "    print('------', k, 'fold ------')\n",
    "    w, loss = ridge_regression(train_y, train_x, lambda_)\n",
    "    pred = train_x @ w\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred < 0] = -1\n",
    "    train_acc.append(np.mean(pred == train_y))\n",
    "    print('Train acc:', np.mean(pred == train_y))\n",
    "    pred = val_x @ w\n",
    "    pred[pred > 0] = 1\n",
    "    pred[pred < 0] = -1\n",
    "    val_acc.append(np.mean(pred == val_y))\n",
    "    print('Validation acc:', np.mean(pred == val_y))\n",
    "\n",
    "print('------ summary ------')\n",
    "print('Average Train acc:', sum(train_acc)/len(train_acc))\n",
    "print('Average Validation acc:', sum(val_acc)/len(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time = 19.818 seconds\n",
      "Gradient Descent: loss = -965749.0705326537\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.0000005\n",
    "\n",
    "\n",
    "# Initialization\n",
    "initial_w = l_gd_ws#np.zeros((tx.shape[1]))\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "l_gd_ws, gd_losses= logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7786692030933378"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_labels_logit(w, tx):\n",
    "    y_pred = sigmoid(np.dot(tx, w))\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    return y_pred\n",
    "# predictions\n",
    "y_pred = predict_labels_logit(l_gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
    "(y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time = 19.793 seconds\n",
      "Gradient Descent: loss = 1097476200.777331\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "lambda_ = 0.001\n",
    "\n",
    "# Initialization\n",
    "initial_w = l_gd_ws#np.random.normal(scale = 1/np.sqrt(tx.shape[0]), size=(tx.shape[1]))\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "l_gd_ws, gd_losses= reg_logistic_regression(y, tx,lambda_, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7793722343989082"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions\n",
    "y_pred = predict_labels_logit(l_gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
    "(y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_logit_loss(y,tx,lambda_,gd_ws)\n",
    "#(sigmoid(tx@gd_ws)<0).sum()\n",
    "#tx@gd_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.35898036e-01  4.36714239e-01 -1.08552886e+00 ...  3.79638208e-01\n",
      "   1.85316930e+00  4.09065362e-01]\n",
      " [-7.16897628e-01 -4.66483417e-01  8.34344563e-02 ...  1.34748188e-02\n",
      "   2.92526652e+00  1.11434713e-02]\n",
      " [-2.93845346e-01  6.69180568e-01 -5.16860831e-02 ...  4.94311036e-03\n",
      "   2.31654982e+00  5.08748581e-04]\n",
      " ...\n",
      " [-3.90801233e-02  1.56653721e+01 -4.25443729e-01 ...  6.72410945e-02\n",
      "   5.28207188e-02  3.90736156e-02]\n",
      " [-9.83018290e-01 -2.88263123e-01 -4.32164038e-01 ...  8.42839357e-02\n",
      "   1.48404451e+00  1.14332379e+01]\n",
      " [ 2.11596036e-01 -3.80665862e-01  2.37009387e-01 ...  5.16061134e-04\n",
      "   3.90509401e-01  5.70755550e-03]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:32<00:00,  1.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-7.37578127e-03,  1.03218455e+00, -1.26879096e-01, -7.58821245e-03,\n",
       "        -6.50856027e-03, -1.43952901e-03,  2.32947897e-02,  7.13720917e-03,\n",
       "        -2.13964680e-02,  1.24781735e-02,  1.31138696e-01,  4.20883612e-03,\n",
       "         8.45112644e-03, -4.43693947e-03, -1.83264673e-03, -9.61180968e-02,\n",
       "         4.87404886e-03, -9.23575047e-04,  3.75859714e-01,  1.05601116e-03,\n",
       "        -1.32052470e-02,  2.55997368e-03, -1.59968998e-03,  1.14561451e-02,\n",
       "         2.66795043e-03,  3.49003764e-04, -2.12900829e-03, -2.32405599e-02,\n",
       "        -3.23050165e-02, -3.69246963e-02, -1.61969676e-05,  9.43686552e-04,\n",
       "        -2.88026539e-03,  3.71721876e-02, -2.05313668e-03,  3.01359501e-03,\n",
       "        -6.94624991e-03, -1.26422817e-02,  4.32765941e-03,  9.13340245e-03,\n",
       "         4.34263214e-03,  4.92183143e-03,  2.67928722e-02, -8.75852039e-03,\n",
       "         4.02006423e-03, -1.82732932e-03,  4.01608429e-03,  4.43914916e-03,\n",
       "         5.03880787e-03,  7.22943279e-03, -2.29663689e-03,  8.60163948e-03,\n",
       "         4.77252147e-03, -6.06624651e-04]),\n",
       " array([-7.43074501e-03,  1.03234853e+00, -1.27366364e-01, -7.59495843e-03,\n",
       "        -6.46761347e-03, -1.48073505e-03,  2.34307597e-02,  7.13218384e-03,\n",
       "        -2.13412139e-02,  1.23675917e-02,  1.31279026e-01,  4.20614496e-03,\n",
       "         8.81584007e-03, -4.49582039e-03, -1.80349044e-03, -9.59127466e-02,\n",
       "         4.92371395e-03, -9.16285704e-04,  3.75943139e-01,  1.04348371e-03,\n",
       "        -1.31459901e-02,  2.56833592e-03, -1.59433469e-03,  1.14507913e-02,\n",
       "         2.67452565e-03,  3.42767371e-04, -2.21771345e-03, -2.37941436e-02,\n",
       "        -2.61959551e-02, -3.72226519e-02,  1.83869397e-05,  1.05174497e-03,\n",
       "        -2.85633773e-03,  3.74211272e-02, -2.07115959e-03,  2.92304781e-03,\n",
       "        -7.15318682e-03, -1.26259359e-02,  4.32684976e-03,  1.02470012e-02,\n",
       "         4.38965923e-03,  4.89695208e-03,  2.60645154e-02, -8.68882395e-03,\n",
       "         3.98813819e-03, -2.47698647e-03,  4.01581328e-03,  4.36772534e-03,\n",
       "         5.05867537e-03,  7.22519613e-03, -2.35988136e-03,  8.63892987e-03,\n",
       "         4.74105795e-03, -9.66787856e-04]),\n",
       " [0.23528896443386,\n",
       "  0.12582715190034008,\n",
       "  0.6004277721463286,\n",
       "  2.2035914896713273,\n",
       "  0.13022771074039255,\n",
       "  0.1243142151801192,\n",
       "  0.12519079482407672,\n",
       "  0.11531522022159665,\n",
       "  0.13239550258249155,\n",
       "  0.11775809396854865,\n",
       "  0.11323525514730019,\n",
       "  0.11561486639593731,\n",
       "  0.11492807562286807,\n",
       "  0.11837180145964993,\n",
       "  0.11414076308231115,\n",
       "  0.11329324236417038,\n",
       "  0.11304255312467562,\n",
       "  0.11301135719771735,\n",
       "  0.11299614608979719,\n",
       "  0.1140674187313934])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xx = build_poly(tx[:, 1:], 2)[:, 1:]\n",
    "print(xx)\n",
    "w_init = np.zeros(xx.shape[1])\n",
    "train_first_feature(tx[:, 0], xx, w_init, 0.0001, 20, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To start, you shall upload text.csv to the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tx_test, ids_test = load_csv_data(\"test.csv\", sub_sample=False)\n",
    "\n",
    "# force meaningless data into the means\n",
    "for i in range(tx_test.shape[1]):\n",
    "    tx_test[:, i][tx_test[:, i] == -999] = tx_mean_meaningful[i]\n",
    "\n",
    "# transform the categorical feature (PRI_jet_num) into onthot features\n",
    "tx_test = np.hstack((tx_test, \n",
    "    (tx_test[:, 22] == 0).astype(np.float64)[:, np.newaxis],\n",
    "    (tx_test[:, 22] == 1).astype(np.float64)[:, np.newaxis],\n",
    "    (tx_test[:, 22] == 2).astype(np.float64)[:, np.newaxis],\n",
    "    (tx_test[:, 22] == 3).astype(np.float64)[:, np.newaxis]))\n",
    "tx_test = np.delete(tx_test, 22, axis = 1)\n",
    "\n",
    "# standardization\n",
    "tx_test_mean = np.mean(tx_test, axis=0)\n",
    "tx_test_std = np.std(tx_test, axis=0)\n",
    "tx_test[:, :-4] = (tx_test[:, :-4] - tx_test_mean[:-4]) / tx_test_std[:-4]\n",
    "\n",
    "# sin, cos term # improvement ~\n",
    "tx_test = np.hstack([tx_test, np.cos(tx_test),np.sin(2*tx_test),np.cos(2*tx_test)])\n",
    "\n",
    "# drop all-zero columns, which may cause tX.T @ tX a singular matrix\n",
    "zero_idx = np.argwhere(np.all(tx_test == 0, axis=0))\n",
    "tx_test = np.delete(tx_test, zero_idx, axis=1)\n",
    "\n",
    "# add the bias term to features\n",
    "bias_term = np.ones([tx_test.shape[0], 1])\n",
    "tx_test = np.hstack([tx_test, bias_term])\n",
    "\n",
    "# drop duplicate columns\n",
    "tx_test = tx_test[:, unique_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "op_axes must be a tuple/list matching the number of ops",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcombinations_with_replacement(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mop_axes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ind)\n",
      "\u001b[0;31mValueError\u001b[0m: op_axes must be a tuple/list matching the number of ops"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for ind in itertools.combinations_with_replacement(np.nditer(tx,op_axes = 0),2):\n",
    "    print(ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8877ca64660a6d07633c323136e70418fcd1020e7ad0cd0d4c67d6a541235ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
