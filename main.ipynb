{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "# from training_procedure import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)\n",
    "\n",
    "# mean = []\n",
    "# for i in range(tx.shape[1]):\n",
    "#     tx[:, i][tx[:, i] == -999] = np.mean(tx[:, i][tx[:, i] != -999])\n",
    "#     mean.append(np.mean(tx[:, i][tx[:, i] != -999]))\n",
    "\n",
    "ind = np.where(tx[:, 22] > 1)\n",
    "tx = tx[tx[:, 22] > 1]\n",
    "\n",
    "# transform the categorical features into one-hot features\n",
    "tx = np.hstack((tx, \n",
    "    (tx[:, 22] == 2).astype(np.float64)[:, np.newaxis],\n",
    "    (tx[:, 22] == 3).astype(np.float64)[:, np.newaxis]))\n",
    "tx = np.delete(tx, 22, axis = 1)\n",
    "\n",
    "# standardization\n",
    "tx_mean = np.mean(tx, axis=0)\n",
    "tx_std = np.std(tx, axis=0)\n",
    "tx[:, :-2] = (tx[:, :-2] - tx_mean[:-2]) / tx_std[:-2]\n",
    "\n",
    "# feature augmentation\n",
    "# quadratic terms\n",
    "n = tx.shape[1]\n",
    "for i in range(n):\n",
    "    new_features = np.multiply(tx[:, i:n], tx[:, 0:n-i])\n",
    "    tx = np.hstack([tx, new_features])\n",
    "\n",
    "# drop all-zero columns, which may cause tx.T @ tx a singular matrix\n",
    "zero_idx = np.argwhere(np.all(tx == 0, axis=0))\n",
    "tx = np.delete(tx, zero_idx, axis=1)\n",
    "\n",
    "# quartic terms # improvement ~1%, but too many features!\n",
    "# new_features = np.multiply(tx[:, n:], tx[:, n:])\n",
    "# nf_mean_1 = np.mean(new_features, axis=0)\n",
    "# nf_std_1 = np.std(new_features, axis=0)\n",
    "# new_features = (new_features - nf_mean_1) / nf_std_1\n",
    "# tx = np.hstack([tx, new_features])\n",
    "\n",
    "# # cubic terms # small improvement ~ 0.16%\n",
    "# new_features = np.power(tx[:, :n-4], 3)\n",
    "# nf_mean_2 = np.mean(new_features, axis=0)\n",
    "# nf_std_2 = np.std(new_features, axis=0)\n",
    "# new_features = (new_features - nf_mean_2) / nf_std_2\n",
    "# tx = np.hstack([tx, new_features])\n",
    "\n",
    "# sin, cos term # improvement ~ 4%\n",
    "tx = np.hstack([tx, np.cos(tx),np.sin(2*tx),np.cos(2*tx)])\n",
    "# cos_, _, _ = standardize(np.cos(tx))\n",
    "# sin_, _, _ = standardize(np.sin(2*tx))\n",
    "# cos__, _, _ = standardize(np.cos(2*tx))\n",
    "# tx = np.hstack([tx, cos_, sin_, cos__])\n",
    "\n",
    "# add the bias term to features\n",
    "bias_term = np.ones([tx.shape[0], 1])\n",
    "tx = np.hstack([tx, bias_term])\n",
    "\n",
    "# drop duplicate columns\n",
    "tx, unique_idx = np.unique(tx, axis=1, return_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72543, 2097), (72543,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.shape, y[ind].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, tx, ids = load_csv_data(\"train.csv\", sub_sample=False)\n",
    "\n",
    "# # change outlier values to the mean without them\n",
    "# mean = []\n",
    "# for i in range(tx.shape[1]):\n",
    "#     tx[:, i][tx[:, i] == -999] = np.mean(tx[:, i][tx[:, i] != -999])\n",
    "#     mean.append(np.mean(tx[:, i][tx[:, i] != -999]))\n",
    "# # transform the categorical features into one-hot features\n",
    "# #tx_ = np.delete(tx, 22, axis = 1)\n",
    "# #tx = np.hstack((tx,tx_**2,tx_**3,np.sin(tx_),np.cos(tx_),np.sin(2*tx_),np.cos(2*tx_),np.sin(tx_)**2,np.cos(tx_)**2,np.sin(0.5*tx_),np.cos(0.5*tx_),\n",
    "# #    (tx[:, 22] == 0).astype(np.float64)[:, np.newaxis],\n",
    "# #    (tx[:, 22] == 1).astype(np.float64)[:, np.newaxis],\n",
    "# #    (tx[:, 22] == 2).astype(np.float64)[:, np.newaxis],\n",
    "# #    (tx[:, 22] == 3).astype(np.float64)[:, np.newaxis]))\n",
    "# ind = np.where(tx[:, 22] > 1)\n",
    "# tx_ = np.delete(tx[tx[:, 22] > 1], 22, axis = 1)\n",
    "# tx__ = np.hstack([tx[tx[:, 22] > 1],tx_[:,0].reshape((tx_.shape[0],1))*tx_,tx_[:,1].reshape((tx_.shape[0],1))*tx_,tx_[:,2].reshape((tx_.shape[0],1))*tx_,tx_[:,3].reshape((tx_.shape[0],1))*tx_,tx_[:,4].reshape((tx_.shape[0],1))*tx_,tx_[:,5].reshape((tx_.shape[0],1))*tx_,tx_[:,6].reshape((tx_.shape[0],1))*tx_,\n",
    "#                 tx_[:,7].reshape((tx_.shape[0],1))*tx_,np.sin(tx_),tx_[:,8].reshape((tx_.shape[0],1))*tx_,\n",
    "#                 tx_[:,9].reshape((tx_.shape[0],1))*tx_,tx_[:,10].reshape((tx_.shape[0],1))*tx_,\n",
    "#                 tx_[:,11].reshape((tx_.shape[0],1))*tx_,tx_[:,12].reshape((tx_.shape[0],1))*tx_,tx_[:,13].reshape((tx_.shape[0],1))*tx_,\n",
    "#                 tx_[:,14].reshape((tx_.shape[0],1))*tx_,tx_[:,15].reshape((tx_.shape[0],1))*tx_,tx_[:,16].reshape((tx_.shape[0],1))*tx_,\n",
    "#                 tx_[:,17].reshape((tx_.shape[0],1))*tx_,tx_[:,18].reshape((tx_.shape[0],1))*tx_,tx_[:,19].reshape((tx_.shape[0],1))*tx_,\n",
    "#                 np.cos(tx_),np.sin(2*tx_),np.cos(2*tx_)])\n",
    "# tx = np.hstack([tx_])\n",
    "# tx = np.delete(tx, 22, axis = 1)\n",
    "\n",
    "# # standardization\n",
    "# tx_mean = np.mean(tx, axis=0)\n",
    "# tx_std = np.std(tx, axis=0)\n",
    "# tx = (tx - tx_mean) / tx_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #y.shape, tx.shape\n",
    "# np.mean(tx, axis = 0).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time = 4.883 seconds\n",
      "Gradient Descent: loss = 0.29985534026201366\n",
      "Gradient descent accuracy: 0.7971961457342541\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.0039\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= least_squares_GD(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))\n",
    "\n",
    "y_pred = tx @ gd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Gradient descent accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation for GD. Maybe No Need...\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "k_fold = 5\n",
    "seed = None\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "max_iters = 10 \n",
    "gammas = np.logspace(-4, -2, 50)\n",
    "\n",
    "# select gamma\n",
    "gamma_ = hyperparameter_tuning_GD(tx, y[ind], k_fold, seed, initial_w, max_iters, gammas)\n",
    "\n",
    "# Start gd\n",
    "gd_ws, gd_losses = least_squares_GD(y[ind], tx, initial_w, max_iters, gamma_)\n",
    "\n",
    "# Print result\n",
    "print(\"Hyperparameter Selection: Gamma = {t}\".format(t=gamma_))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))\n",
    "\n",
    "y_pred = tx @ gd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Gradient descent accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent: execution time = 39.388 seconds\n",
      "Stochastic Gradient Descent: loss = 0.42943024203356345\n",
      "Stochastic gradient descent accuracy: 0.6872199936589333\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gamma = 0.000000015\n",
    "sgd_ws = np.zeros(tx.shape[1])\n",
    "\n",
    "# Initialization\n",
    "initial_w = sgd_ws\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_ws, sgd_losses= least_squares_SGD(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Stochastic Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Stochastic Gradient Descent: loss = {t}\".format(t=sgd_losses))\n",
    "\n",
    "y_pred = tx @ sgd_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Stochastic gradient descent accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares: execution time = 1.957 seconds\n",
      "Least Squares: loss = 0.2414721647739247\n",
      "Least squares accuracy: 0.8477316901699682\n"
     ]
    }
   ],
   "source": [
    "# Start least squares.\n",
    "start_time = datetime.datetime.now()\n",
    "ls_ws, ls_losses= least_squares(y[ind], tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Least Squares: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Least Squares: loss = {t}\".format(t=ls_losses))\n",
    "\n",
    "y_pred = tx @ ls_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Least squares accuracy:', np.mean(y_pred == y[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression: execution time = 2.103 seconds\n",
      "Ridge Regression: loss = 0.241469934786494\n",
      "Ridge Regression accuracy: 0.8477041203148478\n"
     ]
    }
   ],
   "source": [
    "# Start least squares.\n",
    "start_time = datetime.datetime.now()\n",
    "r_ws, r_losses = ridge_regression(y[ind], tx, 1e-17)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge Regression: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Ridge Regression: loss = {t}\".format(t=r_losses))\n",
    "\n",
    "y_pred = tx @ r_ws\n",
    "y_pred[y_pred > 0] = 1\n",
    "y_pred[y_pred < 0] = -1\n",
    "print('Ridge Regression accuracy:', np.mean(y_pred == y[ind]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialization\n",
    "# degree = 1\n",
    "# ratio = 0.8\n",
    "# seed = None\n",
    "# lambdas = np.logspace(-20, -15, 150)\n",
    "\n",
    "# # select lambda\n",
    "# lambda_ = hyperparameter_tuning_ridge_regression(tx, y[ind], degree, ratio, seed, lambdas)\n",
    "\n",
    "# # Start ridge regression\n",
    "# start_time = datetime.datetime.now()\n",
    "# r_ws, r_losses = ridge_regression(y[ind], tx, lambda_)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "# # Print result\n",
    "# exection_time = (end_time - start_time).total_seconds()\n",
    "# print(\"Ridge Regression: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "# print(\"Hyperparameter Selection: Lambda = {t}\".format(t=lambda_))\n",
    "# print(\"Ridge Regression: loss = {t}\".format(t=r_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_k_indices(y, k_fold, seed):\n",
    "#     \"\"\"build k indices for k-fold.\n",
    "#     return dimension: k-fold * interval\n",
    "#     \"\"\"\n",
    "#     num_row = y.shape[0]\n",
    "#     interval = int(num_row / k_fold)\n",
    "#     np.random.seed(seed)\n",
    "#     indices = np.random.permutation(num_row)\n",
    "#     k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)] \n",
    "#     return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross Validation\n",
    "\n",
    "# # Initialization\n",
    "# seed = None\n",
    "# k_fold = 5\n",
    "# k_indices = build_k_indices(y[ind], k_fold, seed)\n",
    "\n",
    "# train_acc = []\n",
    "# val_acc = []\n",
    "# for k in range(k_fold):\n",
    "#     val_y = y[k_indices[k, :]]\n",
    "#     val_x = tx[k_indices[k, :]]\n",
    "#     train_idx = np.vstack([k_indices[:k, :], k_indices[k+1:, :]]).flatten()\n",
    "#     train_y = y[train_idx]\n",
    "#     train_x = tx[train_idx]\n",
    "\n",
    "#     print('------', k, 'fold ------')\n",
    "#     w, loss = ridge_regression(train_y, train_x, lambda_)\n",
    "#     pred = train_x @ w\n",
    "#     pred[pred > 0] = 1\n",
    "#     pred[pred < 0] = -1\n",
    "#     train_acc.append(np.mean(pred == train_y))\n",
    "#     print('Train acc:', np.mean(pred == train_y))\n",
    "#     pred = val_x @ w\n",
    "#     pred[pred > 0] = 1\n",
    "#     pred[pred < 0] = -1\n",
    "#     val_acc.append(np.mean(pred == val_y))\n",
    "#     print('Validation acc:', np.mean(pred == val_y))\n",
    "\n",
    "# print('------ summary ------')\n",
    "# print('Average Train acc:', sum(train_acc)/len(train_acc))\n",
    "# print('Average Validation acc:', sum(val_acc)/len(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 2\n",
    "gamma = 0.0005\n",
    "\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= logistic_regression(y[ind], tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_logit(w, tx):\n",
    "    y_pred = sigmoid(np.dot(tx, w))\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    return y_pred\n",
    "# predictions\n",
    "y_pred = predict_labels_logit(gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
    "(y_pred == y[ind]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 3\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.00001\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_ws, gd_losses= reg_logistic_regression(y[ind], tx,lambda_, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time = {t:.3f} seconds\".format(t=exection_time))\n",
    "print(\"Gradient Descent: loss = {t}\".format(t=gd_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred = predict_labels_logit(gd_ws, tx)\n",
    "\n",
    "# accuracy\n",
    "(y_pred == y[ind]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_logit_loss(y,tx,lambda_,gd_ws)\n",
    "#(sigmoid(tx@gd_ws)<0).sum()\n",
    "#tx@gd_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.35898036e-01  4.36714239e-01 -1.08552886e+00 ...  3.79638208e-01\n",
      "   1.85316930e+00  4.09065362e-01]\n",
      " [-7.16897628e-01 -4.66483417e-01  8.34344563e-02 ...  1.34748188e-02\n",
      "   2.92526652e+00  1.11434713e-02]\n",
      " [-2.93845346e-01  6.69180568e-01 -5.16860831e-02 ...  4.94311036e-03\n",
      "   2.31654982e+00  5.08748581e-04]\n",
      " ...\n",
      " [-3.90801233e-02  1.56653721e+01 -4.25443729e-01 ...  6.72410945e-02\n",
      "   5.28207188e-02  3.90736156e-02]\n",
      " [-9.83018290e-01 -2.88263123e-01 -4.32164038e-01 ...  8.42839357e-02\n",
      "   1.48404451e+00  1.14332379e+01]\n",
      " [ 2.11596036e-01 -3.80665862e-01  2.37009387e-01 ...  5.16061134e-04\n",
      "   3.90509401e-01  5.70755550e-03]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:32<00:00,  1.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-7.37578127e-03,  1.03218455e+00, -1.26879096e-01, -7.58821245e-03,\n",
       "        -6.50856027e-03, -1.43952901e-03,  2.32947897e-02,  7.13720917e-03,\n",
       "        -2.13964680e-02,  1.24781735e-02,  1.31138696e-01,  4.20883612e-03,\n",
       "         8.45112644e-03, -4.43693947e-03, -1.83264673e-03, -9.61180968e-02,\n",
       "         4.87404886e-03, -9.23575047e-04,  3.75859714e-01,  1.05601116e-03,\n",
       "        -1.32052470e-02,  2.55997368e-03, -1.59968998e-03,  1.14561451e-02,\n",
       "         2.66795043e-03,  3.49003764e-04, -2.12900829e-03, -2.32405599e-02,\n",
       "        -3.23050165e-02, -3.69246963e-02, -1.61969676e-05,  9.43686552e-04,\n",
       "        -2.88026539e-03,  3.71721876e-02, -2.05313668e-03,  3.01359501e-03,\n",
       "        -6.94624991e-03, -1.26422817e-02,  4.32765941e-03,  9.13340245e-03,\n",
       "         4.34263214e-03,  4.92183143e-03,  2.67928722e-02, -8.75852039e-03,\n",
       "         4.02006423e-03, -1.82732932e-03,  4.01608429e-03,  4.43914916e-03,\n",
       "         5.03880787e-03,  7.22943279e-03, -2.29663689e-03,  8.60163948e-03,\n",
       "         4.77252147e-03, -6.06624651e-04]),\n",
       " array([-7.43074501e-03,  1.03234853e+00, -1.27366364e-01, -7.59495843e-03,\n",
       "        -6.46761347e-03, -1.48073505e-03,  2.34307597e-02,  7.13218384e-03,\n",
       "        -2.13412139e-02,  1.23675917e-02,  1.31279026e-01,  4.20614496e-03,\n",
       "         8.81584007e-03, -4.49582039e-03, -1.80349044e-03, -9.59127466e-02,\n",
       "         4.92371395e-03, -9.16285704e-04,  3.75943139e-01,  1.04348371e-03,\n",
       "        -1.31459901e-02,  2.56833592e-03, -1.59433469e-03,  1.14507913e-02,\n",
       "         2.67452565e-03,  3.42767371e-04, -2.21771345e-03, -2.37941436e-02,\n",
       "        -2.61959551e-02, -3.72226519e-02,  1.83869397e-05,  1.05174497e-03,\n",
       "        -2.85633773e-03,  3.74211272e-02, -2.07115959e-03,  2.92304781e-03,\n",
       "        -7.15318682e-03, -1.26259359e-02,  4.32684976e-03,  1.02470012e-02,\n",
       "         4.38965923e-03,  4.89695208e-03,  2.60645154e-02, -8.68882395e-03,\n",
       "         3.98813819e-03, -2.47698647e-03,  4.01581328e-03,  4.36772534e-03,\n",
       "         5.05867537e-03,  7.22519613e-03, -2.35988136e-03,  8.63892987e-03,\n",
       "         4.74105795e-03, -9.66787856e-04]),\n",
       " [0.23528896443386,\n",
       "  0.12582715190034008,\n",
       "  0.6004277721463286,\n",
       "  2.2035914896713273,\n",
       "  0.13022771074039255,\n",
       "  0.1243142151801192,\n",
       "  0.12519079482407672,\n",
       "  0.11531522022159665,\n",
       "  0.13239550258249155,\n",
       "  0.11775809396854865,\n",
       "  0.11323525514730019,\n",
       "  0.11561486639593731,\n",
       "  0.11492807562286807,\n",
       "  0.11837180145964993,\n",
       "  0.11414076308231115,\n",
       "  0.11329324236417038,\n",
       "  0.11304255312467562,\n",
       "  0.11301135719771735,\n",
       "  0.11299614608979719,\n",
       "  0.1140674187313934])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xx = build_poly(tx[:, 1:], 2)[:, 1:]\n",
    "print(xx)\n",
    "w_init = np.zeros(xx.shape[1])\n",
    "train_first_feature(tx[:, 0], xx, w_init, 0.0001, 20, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To start, you shall upload text.csv to the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tx_test, ids_test = load_csv_data(\"test.csv\", sub_sample=False)\n",
    "\n",
    "# delete uninformative rows with '-999'\n",
    "tx_test = tx_test[tx_test[:, 22] > 1]\n",
    "\n",
    "# transform the categorical feature (PRI_jet_num) into onthot features\n",
    "tx_test = np.hstack((tx_test, \n",
    "    (tx_test[:, 22] == 2).astype(np.float64)[:, np.newaxis],\n",
    "    (tx_test[:, 22] == 3).astype(np.float64)[:, np.newaxis]))\n",
    "tx_test = np.delete(tx_test, 22, axis = 1)\n",
    "\n",
    "# standardization\n",
    "tx_test_mean = np.mean(tx_test, axis=0)\n",
    "tx_test_std = np.std(tx_test, axis=0)\n",
    "tx_test[:, :-2] = (tx_test[:, :-2] - tx_test_mean[:-2]) / tx_test_std[:-2]\n",
    "\n",
    "# feature augmentation\n",
    "# quadratic terms\n",
    "n = tx_test.shape[1]\n",
    "for i in range(n):\n",
    "    new_features = np.multiply(tx_test[:, i:n], tx_test[:, 0:n-i])\n",
    "    tx_test = np.hstack([tx_test, new_features])\n",
    "\n",
    "# drop all-zero columns, which may cause tX.T @ tX a singular matrix\n",
    "zero_idx = np.argwhere(np.all(tx_test == 0, axis=0))\n",
    "tx_test = np.delete(tx_test, zero_idx, axis=1)\n",
    "\n",
    "# sin, cos term \n",
    "tx_test = np.hstack([tx_test, np.cos(tx_test),np.sin(2*tx_test),np.cos(2*tx_test)])\n",
    "\n",
    "# add the bias term to features\n",
    "bias_term = np.ones([tx_test.shape[0], 1])\n",
    "tx_test = np.hstack([tx_test, bias_term])\n",
    "\n",
    "# drop duplicate columns\n",
    "tx_test = tx_test[:, unique_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "OUTPUT_PATH = 'data/pred_GD.csv'\n",
    "y_pred = predict(gd_ws, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "\n",
    "OUTPUT_PATH = 'data/pred_ls.csv'\n",
    "y_pred = predict(ls_ws, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "\n",
    "OUTPUT_PATH = 'data/pred_ridge.csv'\n",
    "y_pred = predict(r_ws, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8877ca64660a6d07633c323136e70418fcd1020e7ad0cd0d4c67d6a541235ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
